import keras
from keras.datasets import cifar10
from keras.layers import Conv2D, BatchNormalization, Activation, add, MaxPooling2D, AveragePooling2D, Dense, Flatten
from keras.models import Input, Model
from keras.optimizers import Adam
from keras.preprocessing.image import ImageDataGenerator


# group of 2 conv layers with shortcut connections in between 
def layer_group(x, filters, increase_dim = False):
    shortcut = x
    
    if increase_dim:
        shortcut = Conv2D(filters = filters, kernel_size = [1,1], strides = [2,2], padding = 'same')(shortcut)
        x = MaxPooling2D(pool_size = [2,2])(x)
    
    output = Conv2D(filters = filters, kernel_size = [3,3], padding = 'same')(x)
    output = BatchNormalization()(output)
    output = Activation('relu')(output)
    
    output = Conv2D(filters = filters, kernel_size = [3,3], padding = 'same')(x)
    output = BatchNormalization()(output)
    output = Activation('relu')(output)
    
    return add([shortcut, output])
    
def RNN(shape):
    input = Input(shape)
    layers = Conv2D(filters = 64, kernel_size = [7,7], padding = 'same')(input)

    layers = layer_group(layers, 64)
    layers = layer_group(layers, 64)
    layers = layer_group(layers, 64)
    
    layers = layer_group(layers, 128, increase_dim = True)
    layers = layer_group(layers, 128)
    layers = layer_group(layers, 128)
    layers = layer_group(layers, 128)
    
    layers = layer_group(layers, 256, increase_dim = True)
    layers = layer_group(layers, 256)
    layers = layer_group(layers, 256)
    layers = layer_group(layers, 256)
    layers = layer_group(layers, 256)
    layers = layer_group(layers, 256)
    
    layers = layer_group(layers, 512, increase_dim = True)
    layers = layer_group(layers, 512)
    layers = layer_group(layers, 512)
    
    layers = AveragePooling2D(pool_size=(2,2))(layers)
    layers = Flatten()(layers)
    layers = Dense(units=10,activation="softmax")(layers)

    model = Model(inputs=input,outputs=layers)

    return model

(x_train, y_train) , (x_test, y_test) = cifar10.load_data()

#subtract mean
x_train = x_train - x_train.mean()
x_test = x_test - x_test.mean()

#normalize data to values between 0 and 1
x_train = x_train.astype('float32') / 255
x_test = x_test.astype('float32') / 255

#image augmentation from https://appliedmachinelearning.blog/2018/03/24/achieving-90-accuracy-in-object-recognition-task-on-cifar-10-dataset-with-keras-convolutional-neural-networks/
datagen = ImageDataGenerator( rotation_range=90,
                 width_shift_range=0.1, height_shift_range=0.1,
                 horizontal_flip=True)

datagen.fit(x_train)

#Encode the labels to vectors
y_train = keras.utils.to_categorical(y_train,10)
y_test = keras.utils.to_categorical(y_test,10)


input_shape = (32,32,3)
model = RNN(input_shape)
model.summary()

# optimizer and learning rate from https://medium.com/octavian-ai/which-optimizer-and-learning-rate-should-i-use-for-deep-learning-5acb418f9b2
model.compile(optimizer=Adam(0.001), loss="categorical_crossentropy", metrics=["accuracy"])

epochs = 30
steps_per_epoch = 500

# Fit the model on the batches generated by datagen.flow().
model.fit_generator(datagen.flow(x_train, y_train, batch_size=128),
                    validation_data=[x_test, y_test],
                    epochs=epochs,steps_per_epoch=steps_per_epoch, verbose=1, workers=4)

accuracy = model.evaluate(x=x_test,y=y_test,batch_size=128)
print("Val Accuracy", accuracy[1])
model.save("cifar10model.h5")